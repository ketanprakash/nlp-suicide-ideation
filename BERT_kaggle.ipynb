{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8054753,"sourceType":"datasetVersion","datasetId":4750532}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport tensorflow as tf\nimport pandas as pd\nfrom transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load dataset\ndata = pd.read_csv('/kaggle/input/suicide-ideation-clean/clean_text.csv')  # Adjust this to the path of your dataset\n\n# Assuming 'text' is the column with text data and 'label' is the column with labels\nX = data['clean_text'].astype(str)\ny = data['class']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Convert data to InputExamples\ndef convert_examples_to_tf_dataset(examples, labels, tokenizer, max_length=128):\n    input_features = []\n\n    for example, label in zip(examples, labels):\n        input_dict = tokenizer.encode_plus(\n            example,\n            add_special_tokens=True,\n            max_length=max_length,  # Max length of the text that can go to BERT\n            return_attention_mask=True,\n            pad_to_max_length=True,  # Add [PAD] tokens\n            truncation=True\n        )\n        input_ids, attention_mask = input_dict['input_ids'], input_dict['attention_mask']\n\n        input_features.append(\n            InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=None, label=label)\n        )\n\n    def gen():\n        for feature in input_features:\n            yield (\n                {\n                    \"input_ids\": feature.input_ids,\n                    \"attention_mask\": feature.attention_mask,\n                },\n                feature.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int64),\n        (\n            {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None])},\n            tf.TensorShape([]),\n        ),\n    )\n\n# Prepare the datasets\ntrain_dataset = convert_examples_to_tf_dataset(list(X_train), list(y_train), tokenizer)\ntest_dataset = convert_examples_to_tf_dataset(list(X_test), list(y_test), tokenizer)\n\n# Determine the number of labels\nnum_labels = len(label_encoder.classes_)\n\n# Load pre-trained BERT model\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n# Fine-tune the model\nmodel.fit(train_dataset.shuffle(100).batch(32), epochs=3, batch_size=32, validation_data=test_dataset.batch(32))\n\n# Evaluate the model\nresult = model.evaluate(test_dataset.batch(32), return_dict=True)\nprint(f\"Test loss: {result['loss']}\")\nprint(f\"Test accuracy: {result['accuracy']}\")\n\n# Save the model\nmodel.save_pretrained(\"/kaggle/working/bert_finetuned_classification\")\n\n# To load the model later\n# model = TFBertForSequenceClassification.from_pretrained(\"./bert_finetuned_classification\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T20:37:57.440782Z","iopub.execute_input":"2024-04-07T20:37:57.441146Z","iopub.status.idle":"2024-04-08T01:10:35.296546Z","shell.execute_reply.started":"2024-04-07T20:37:57.441112Z","shell.execute_reply":"2024-04-08T01:10:35.295732Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-07 20:37:59.370062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-07 20:37:59.370191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-07 20:37:59.516767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a37d6773fd664b97a902902eee7777e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343d53d0899c420fb42b9bb84d917015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c303c292664448f4a414fdba4f0625ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c31d336d644428d9623fe36b8054e39"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f1ed382010410d845fbcb7bef3e86f"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nWARNING: AutoGraph could not transform <function infer_framework at 0x7fb4e3c2d480> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712522954.936151      93 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"5802/5802 [==============================] - 5191s 878ms/step - loss: 0.1510 - accuracy: 0.9441 - val_loss: 0.1366 - val_accuracy: 0.9498\nEpoch 2/3\n5802/5802 [==============================] - 5084s 876ms/step - loss: 0.0993 - accuracy: 0.9640 - val_loss: 0.1276 - val_accuracy: 0.9549\nEpoch 3/3\n5802/5802 [==============================] - 5081s 876ms/step - loss: 0.0713 - accuracy: 0.9750 - val_loss: 0.1456 - val_accuracy: 0.9551\n1451/1451 [==============================] - 407s 281ms/step - loss: 0.1456 - accuracy: 0.9551\nTest loss: 0.14560937881469727\nTest accuracy: 0.9551222920417786\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:10:35.298264Z","iopub.execute_input":"2024-04-08T01:10:35.298575Z","iopub.status.idle":"2024-04-08T01:10:47.814869Z","shell.execute_reply.started":"2024-04-08T01:10:35.298539Z","shell.execute_reply":"2024-04-08T01:10:47.813902Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\nimport numpy as np\n\n# Predictions\npreds = model.predict(test_dataset.batch(32), verbose=1)\npred_labels = np.argmax(preds.logits, axis=1)\n\n# True labels\ntrue_labels = list(y_test)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Accuracy\naccuracy = accuracy_score(true_labels, pred_labels)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n\n# Recall\nrecall = recall_score(true_labels, pred_labels, average='binary')\nprint(\"Recall: {:.2f}\".format(recall))\n\n# Precision\nprecision = precision_score(true_labels, pred_labels, average='binary')\nprint(\"Precision: {:.2f}\".format(precision))\n\n# F1-score\nf1 = f1_score(true_labels, pred_labels, average='binary')\nprint(\"F1-score: {:.2f}\".format(f1))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:10:47.816227Z","iopub.execute_input":"2024-04-08T01:10:47.816542Z","iopub.status.idle":"2024-04-08T01:17:44.127127Z","shell.execute_reply.started":"2024-04-08T01:10:47.816513Z","shell.execute_reply":"2024-04-08T01:17:44.126165Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1451/1451 [==============================] - 415s 279ms/step\nConfusion Matrix:\n [[22231  1056]\n [ 1027 22101]]\nAccuracy: 95.51%\nRecall: 0.96\nPrecision: 0.95\nF1-score: 0.95\n","output_type":"stream"}]}]}